{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755f1664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import Nio\n",
    "import datetime\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0e7f7",
   "metadata": {},
   "source": [
    "# Dynamical Data Processing\n",
    "\n",
    "This notebook calculates the target predictor variables from the downloaded subset of the NCEP FNL Reanalysis (1999-) dataset. The subset contains, from 1999-08-01 00:00 to 2019-12-31 23:00 and covering longitudes 53E to 164W, latitudes 8S to 55N, the following variables:\n",
    "\n",
    "- geopotential height, 500 mb\n",
    "\n",
    "- u- and v-components of wind, at 1000mb, 850mb, 500mb and 200mb levels\n",
    "\n",
    "- relative humidity, at 300-500 and 750-800mb\n",
    "\n",
    "- temperature, at surface and 200 mb\n",
    "\n",
    "- potential temperature, at sigma level 0.995\n",
    "\n",
    "- absolute vorticity, at 850mb\n",
    "\n",
    "This notebook takes, most notably, PyNIO as a prerequisite, and works only with a Python 2 kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e777e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "\n",
    "def save_dict_to_pickle(data, filename):\n",
    "    '''\n",
    "    Takes in a dictionary and a full filename (with extensions) and writes the dictionary to the specified file as pickles.\n",
    "    '''\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "def save_dict_to_csv(data, filename):\n",
    "    '''\n",
    "    Takes in dictionary and a full filename (with extensions) and writes the dictionary to the specified file as a CSV (no header).\n",
    "    '''\n",
    "    with open(filename, 'wb') as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(data.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db65c60",
   "metadata": {},
   "source": [
    "### Geopotential height\n",
    "\n",
    "Preprocessing targets:\n",
    "- WNPSH intensity index: 10°–60°N, 100°E–180°, >5870gpm, average value\n",
    "\n",
    "- WNPSH area index: 10°–60°N, 100°E–180°, >5870gpm, grids count\n",
    "\n",
    "- WNPSH westward extension index: 5870gpm contour westmost longitude\n",
    "\n",
    "- westerly index: H35 - H55 from 100E to 180E\n",
    "\n",
    "all calculated for each available time step.\n",
    "\n",
    "Missing value considerations:\n",
    "- if area index == 0,\n",
    "    then no intensity/westward extension indices\n",
    "    \n",
    "- there are disgustingly many missing *files*.\n",
    "  - 534 GRIB1 files and 17634 GRIB2 files are available, adding up to a total of 18168 files (NCEP FNL subset API claims 11542 only).\n",
    "  \n",
    "  - This will be the upper limit of the final dataset size.\n",
    "  \n",
    "Because of the missing files issue, alternative data sources may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "72e793ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bookkeeping\n",
    "wnpsh_area_indices = dict()\n",
    "wnpsh_intensity_indices = dict()\n",
    "wnpsh_extension_indices = dict()\n",
    "westerly_indices = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8cf9c24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1335 data files out of 13000 expected. 11665 files are missing. Current year 2008\n",
      "Completed 2335 data files out of 14000 expected. 11665 files are missing. Current year 2009\n",
      "Completed 3335 data files out of 15000 expected. 11665 files are missing. Current year 2009\n",
      "Completed 4335 data files out of 16000 expected. 11665 files are missing. Current year 2010\n",
      "Completed 5335 data files out of 17000 expected. 11665 files are missing. Current year 2011\n",
      "Completed 6335 data files out of 18000 expected. 11665 files are missing. Current year 2011\n",
      "Completed 7335 data files out of 19000 expected. 11665 files are missing. Current year 2012\n",
      "Completed 8335 data files out of 20000 expected. 11665 files are missing. Current year 2013\n",
      "Completed 9335 data files out of 21000 expected. 11665 files are missing. Current year 2013\n",
      "Completed 10335 data files out of 22000 expected. 11665 files are missing. Current year 2014\n",
      "Completed 11335 data files out of 23000 expected. 11665 files are missing. Current year 2015\n",
      "Completed 12335 data files out of 24000 expected. 11665 files are missing. Current year 2016\n",
      "Completed 13335 data files out of 25000 expected. 11665 files are missing. Current year 2016\n",
      "Completed 14335 data files out of 26000 expected. 11665 files are missing. Current year 2017\n",
      "Completed 15335 data files out of 27000 expected. 11665 files are missing. Current year 2018\n",
      "Completed 16335 data files out of 28000 expected. 11665 files are missing. Current year 2018\n",
      "Completed 17335 data files out of 29000 expected. 11665 files are missing. Current year 2019\n"
     ]
    }
   ],
   "source": [
    "# start from this date\n",
    "data_time = datetime.datetime(1999, 8, 1, 0, 0, 0)\n",
    "# advance at 6 hour time steps\n",
    "delta_fixee = datetime.timedelta(hours=6)\n",
    "\n",
    "num_ok = 0\n",
    "num_expected = 0\n",
    "num_ng = 0\n",
    "\n",
    "# main loop\n",
    "# repeat until all data are exhausted (time limit reached)\n",
    "while data_time < datetime.datetime(2019, 12, 31, 23, 0, 0):\n",
    "    \n",
    "    num_expected += 1\n",
    "\n",
    "    # GRIB1 files are available until 6 Dec 2007 0600 (UTC), \n",
    "    # the variable we need in that case is HGT_3_ISBL\n",
    "    # modify accordingly for GRIB2\n",
    "    extension = \"grib1\"\n",
    "    var_name = \"HGT_3_ISBL\"\n",
    "    latlon_suffix = 3 # grib1: lat_3, grib2: lat_0\n",
    "    if data_time > datetime.datetime(2007, 12, 6, 6, 0, 0):\n",
    "        extension = \"grib2\"\n",
    "        var_name = \"HGT_P0_L100_GLL0\"\n",
    "        latlon_suffix = 0\n",
    "\n",
    "    # generate target file name\n",
    "    time_string = data_time.strftime(\"%Y%m%d_%H_%M\")\n",
    "    filename = \"./geop/fnl_{0}.{1}\".format(time_string, extension)\n",
    "    # check file existence: if no such file, go to next\n",
    "    if not os.path.isfile(filename):\n",
    "        # print \"Cannot open file {0}\".format(filename)\n",
    "        num_ng += 1\n",
    "        data_time += delta_fixee\n",
    "        continue\n",
    "        \n",
    "    # open files and calculate target predictors\n",
    "    \n",
    "    # print \"Opening file {0}\".format(filename)\n",
    "    geop = Nio.open_file(filename, mode='r')\n",
    "\n",
    "    wnpsh = geop.variables[var_name][\"lat_{0}|10:60 lon_{0}|100:180\".format(latlon_suffix)]\n",
    "    area_index = np.count_nonzero(wnpsh > 5870.0)\n",
    "    # print \"Area index:\", area_index\n",
    "    intensity_index = 0 if area_index == 0 else np.average(wnpsh, weights=(wnpsh > 5870.0))    \n",
    "    # print \"Intensity index:\", intensity_index\n",
    "\n",
    "    everything = geop.variables[var_name].get_value()\n",
    "    extension_index = 0 if area_index == 0 else min(np.argwhere(np.any(everything > 5870.0, axis=0))) + 53\n",
    "    extension_index = int(extension_index)\n",
    "    #print \"Westward extension index:\", int(extension_index)\n",
    "\n",
    "    westerly = geop.variables[var_name][\"lat_{0}|35,55 lon_{0}|100:180\".format(latlon_suffix)]\n",
    "    westerly_index = np.average(westerly[0] - westerly[1])\n",
    "    # print \"Westerly index:\", westerly_index\n",
    "\n",
    "    geop.close()\n",
    "    \n",
    "    # bookkeeping\n",
    "    wnpsh_area_indices[time_string] = area_index\n",
    "    wnpsh_intensity_indices[time_string] = intensity_index\n",
    "    wnpsh_extension_indices[time_string] = extension_index\n",
    "    westerly_indices[time_string] = westerly_index\n",
    "    \n",
    "    num_ok += 1\n",
    "    data_time += delta_fixee\n",
    "    \n",
    "    if num_expected % 1000 == 0:\n",
    "        print \"Completed {0} data files out of {1} expected. {2} files are missing. Current year {3}\".format(num_ok, num_expected, num_ng, data_time.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7f8a9908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11665 18167 29832\n",
      "18167\n",
      "18167\n",
      "18167\n",
      "18167\n"
     ]
    }
   ],
   "source": [
    "print num_ng, num_ok, num_expected\n",
    "\n",
    "print len(wnpsh_area_indices)\n",
    "print len(wnpsh_intensity_indices)\n",
    "print len(wnpsh_extension_indices)\n",
    "print len(westerly_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "466b84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_csv(wnpsh_area_indices, \"wnpsh_area_indices.csv\")\n",
    "save_dict_to_csv(wnpsh_intensity_indices, \"wnpsh_intensity_indices.csv\")\n",
    "save_dict_to_csv(wnpsh_extension_indices, \"wnpsh_extension_indices.csv\")\n",
    "save_dict_to_csv(westerly_indices, \"westerly_indices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866bd9a4",
   "metadata": {},
   "source": [
    "### Winds, u- and v- components, Part 1\n",
    "\n",
    "This part focuses on the items possible to compute in advance, namely:\n",
    "\n",
    "- Hong Kong surface winds (1000mb level as proxy):\n",
    "  - Given HK's coordinates are 22.30N and 114.17E, the nearest 4 grid points should be selected, and then have their values averaged by some appropriate weights, i.e. by interpolation.\n",
    "  \n",
    "- East Asia Summer Monsoon (EASM) index:\n",
    "  - U850 in (5°–15°N, 90°–130°E) minus U850 in (22.5°–32.5°N, 110°–140°E) \n",
    "  \n",
    "  - due to the limited resolution of the data grid, the latter term will be taken for 23N to 33N instead. \n",
    "  \n",
    "Note that this time around there are 11540 GRIB1 files and 17634 GRIB2 files (total 29174) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d670c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bookkeeping\n",
    "hk_u_winds = dict()\n",
    "hk_v_winds = dict()\n",
    "easm_indices = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a78949e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 859 data files out of 1000 expected. 141 files are missing. Current year 2000\n",
      "Completed 1853 data files out of 2000 expected. 147 files are missing. Current year 2000\n",
      "Completed 2407 data files out of 3000 expected. 593 files are missing. Current year 2001\n",
      "Completed 3361 data files out of 4000 expected. 639 files are missing. Current year 2002\n",
      "Completed 4361 data files out of 5000 expected. 639 files are missing. Current year 2003\n",
      "Completed 5361 data files out of 6000 expected. 639 files are missing. Current year 2003\n",
      "Completed 6361 data files out of 7000 expected. 639 files are missing. Current year 2004\n",
      "Completed 7361 data files out of 8000 expected. 639 files are missing. Current year 2005\n",
      "Completed 8360 data files out of 9000 expected. 640 files are missing. Current year 2005\n",
      "Completed 9360 data files out of 10000 expected. 640 files are missing. Current year 2006\n",
      "Completed 10345 data files out of 11000 expected. 655 files are missing. Current year 2007\n",
      "Completed 11342 data files out of 12000 expected. 658 files are missing. Current year 2007\n",
      "Completed 12342 data files out of 13000 expected. 658 files are missing. Current year 2008\n",
      "Completed 13342 data files out of 14000 expected. 658 files are missing. Current year 2009\n",
      "Completed 14342 data files out of 15000 expected. 658 files are missing. Current year 2009\n",
      "Completed 15342 data files out of 16000 expected. 658 files are missing. Current year 2010\n",
      "Completed 16342 data files out of 17000 expected. 658 files are missing. Current year 2011\n",
      "Completed 17342 data files out of 18000 expected. 658 files are missing. Current year 2011\n",
      "Completed 18342 data files out of 19000 expected. 658 files are missing. Current year 2012\n",
      "Completed 19342 data files out of 20000 expected. 658 files are missing. Current year 2013\n",
      "Completed 20342 data files out of 21000 expected. 658 files are missing. Current year 2013\n",
      "Completed 21342 data files out of 22000 expected. 658 files are missing. Current year 2014\n",
      "Completed 22342 data files out of 23000 expected. 658 files are missing. Current year 2015\n",
      "Completed 23342 data files out of 24000 expected. 658 files are missing. Current year 2016\n",
      "Completed 24342 data files out of 25000 expected. 658 files are missing. Current year 2016\n",
      "Completed 25342 data files out of 26000 expected. 658 files are missing. Current year 2017\n",
      "Completed 26342 data files out of 27000 expected. 658 files are missing. Current year 2018\n",
      "Completed 27342 data files out of 28000 expected. 658 files are missing. Current year 2018\n",
      "Completed 28342 data files out of 29000 expected. 658 files are missing. Current year 2019\n"
     ]
    }
   ],
   "source": [
    "# start from this date\n",
    "data_time = datetime.datetime(1999, 8, 1, 0, 0, 0)\n",
    "# advance at 6 hour time steps\n",
    "delta_fixee = datetime.timedelta(hours=6)\n",
    "\n",
    "num_ok = 0\n",
    "num_expected = 0\n",
    "num_ng = 0\n",
    "\n",
    "# main loop\n",
    "# repeat until all data are exhausted (time limit reached)\n",
    "while data_time < datetime.datetime(2019, 12, 31, 23, 0, 0):\n",
    "    \n",
    "    num_expected += 1\n",
    "\n",
    "    # GRIB1 files are available until 6 Dec 2007 0600 (UTC), \n",
    "    # the variables we need in that case are U_GRD_3_ISBL and V_GRD_3_ISBL\n",
    "    # modify accordingly for GRIB2\n",
    "    extension = \"grib1\"\n",
    "    u_var_name = \"U_GRD_3_ISBL\"\n",
    "    v_var_name = \"V_GRD_3_ISBL\"\n",
    "    latlon_suffix = 3 # grib1: lat_3, grib2: lat_0\n",
    "    if data_time > datetime.datetime(2007, 12, 6, 6, 0, 0):\n",
    "        extension = \"grib2\"\n",
    "        u_var_name = \"UGRD_P0_L100_GLL0\"\n",
    "        v_var_name = \"VGRD_P0_L100_GLL0\"\n",
    "        latlon_suffix = 0\n",
    "\n",
    "    # generate target file name\n",
    "    time_string = data_time.strftime(\"%Y%m%d_%H_%M\")\n",
    "    filename = \"./wind/fnl_{0}.{1}\".format(time_string, extension)\n",
    "    # check file existence: if no such file, go to next\n",
    "    if not os.path.isfile(filename):\n",
    "        # print \"Cannot open file {0}\".format(filename)\n",
    "        num_ng += 1\n",
    "        data_time += delta_fixee\n",
    "        continue\n",
    "\n",
    "    # open files and calculate target predictors\n",
    "\n",
    "    # print \"Opening file {0}\".format(filename)\n",
    "    wind = Nio.open_file(filename, mode='r')\n",
    "\n",
    "    # hong kong u wind\n",
    "    hk_u = wind.variables[u_var_name][\"lat_{0}|22.30i lon_{0}|114.17i lv_ISBL0|1000\".format(latlon_suffix)]\n",
    "    # print \"Hong Kong u-wind:\", hk_u\n",
    "\n",
    "    # hong kong v wind\n",
    "    hk_v = wind.variables[v_var_name][\"lat_{0}|22.30i lon_{0}|114.17i lv_ISBL0|1000\".format(latlon_suffix)]\n",
    "    # print \"Hong Kong v-wind:\", hk_v\n",
    "\n",
    "    # easm index\n",
    "    u850_1 = wind.variables[u_var_name][\"lv_ISBL0|850 lon_{0}|90:130\".format(latlon_suffix)][(8+5):(8+15),:]\n",
    "    u850_2 = wind.variables[u_var_name][\"lv_ISBL0|850 lon_{0}|110:140\".format(latlon_suffix)][(8+23):(8+33),:]\n",
    "    easm_idx = np.average(u850_1) - np.average(u850_2)\n",
    "    # print \"EASM index:\", easm_idx\n",
    "\n",
    "    wind.close()\n",
    "    \n",
    "    # bookkeeping\n",
    "    hk_u_winds[time_string] = hk_u\n",
    "    hk_v_winds[time_string] = hk_v\n",
    "    easm_indices[time_string] = easm_idx\n",
    "    \n",
    "    num_ok += 1\n",
    "    data_time += delta_fixee\n",
    "    \n",
    "    if num_expected % 1000 == 0:\n",
    "        print \"Completed {0} data files out of {1} expected. {2} files are missing. Current year {3}\".format(num_ok, num_expected, num_ng, data_time.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3fd7d717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658 29174 29832\n",
      "29174\n",
      "29174\n",
      "29174\n"
     ]
    }
   ],
   "source": [
    "print num_ng, num_ok, num_expected\n",
    "\n",
    "print len(hk_u_winds)\n",
    "print len(hk_v_winds)\n",
    "print len(easm_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c4d147f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_csv(hk_u_winds, \"hk_u_winds.csv\")\n",
    "save_dict_to_csv(hk_v_winds, \"hk_v_winds.csv\")\n",
    "save_dict_to_csv(easm_indices, \"easm_indices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8fef4",
   "metadata": {},
   "source": [
    "### Read best track dataset\n",
    "\n",
    "This is to calculate the rest of the variables. The temporal subset 1999-08-01 00:00 to 2019-12-31 23:00 shall be selected. Then, for each TC (identified by SID) and each time in the TC sequence, we calculate the predictors if the corresponding file(s) exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e2318a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53791 entries, 0 to 53790\n",
      "Data columns (total 25 columns):\n",
      "SID               53791 non-null object\n",
      "SEASON            53791 non-null int32\n",
      "NAME              53791 non-null object\n",
      "ISO_TIME_MONTH    53791 non-null int32\n",
      "ISO_TIME_DAY      53791 non-null int32\n",
      "ISO_TIME_HOUR     53791 non-null int32\n",
      "ISO_TIME_MIN      53791 non-null int32\n",
      "USA_LAT           53791 non-null float32\n",
      "USA_LON           53791 non-null float32\n",
      "USA_WIND          53791 non-null int32\n",
      "USA_PRES          53791 non-null int32\n",
      "USA_R34_NE        53791 non-null int32\n",
      "USA_R34_SE        53791 non-null int32\n",
      "USA_R34_SW        53791 non-null int32\n",
      "USA_R34_NW        53791 non-null int32\n",
      "USA_R50_NE        53791 non-null int32\n",
      "USA_R50_SE        53791 non-null int32\n",
      "USA_R50_SW        53791 non-null int32\n",
      "USA_R50_NW        53791 non-null int32\n",
      "USA_R64_NE        53791 non-null int32\n",
      "USA_R64_SE        53791 non-null int32\n",
      "USA_R64_SW        53791 non-null int32\n",
      "USA_R64_NW        53791 non-null int32\n",
      "STORM_SPEED       53791 non-null int32\n",
      "STORM_DIR         53791 non-null int32\n",
      "dtypes: float32(2), int32(21), object(2)\n",
      "memory usage: 5.5+ MB\n"
     ]
    }
   ],
   "source": [
    "best_track = np.load(\"../best_track.npy\", allow_pickle=True)\n",
    "best_track = pd.DataFrame(best_track)\n",
    "best_track.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50476fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEASON</th>\n",
       "      <th>ISO_TIME_MONTH</th>\n",
       "      <th>ISO_TIME_DAY</th>\n",
       "      <th>ISO_TIME_HOUR</th>\n",
       "      <th>ISO_TIME_MIN</th>\n",
       "      <th>USA_LAT</th>\n",
       "      <th>USA_LON</th>\n",
       "      <th>USA_WIND</th>\n",
       "      <th>USA_PRES</th>\n",
       "      <th>USA_R34_NE</th>\n",
       "      <th>...</th>\n",
       "      <th>USA_R50_NE</th>\n",
       "      <th>USA_R50_SE</th>\n",
       "      <th>USA_R50_SW</th>\n",
       "      <th>USA_R50_NW</th>\n",
       "      <th>USA_R64_NE</th>\n",
       "      <th>USA_R64_SE</th>\n",
       "      <th>USA_R64_SW</th>\n",
       "      <th>USA_R64_NW</th>\n",
       "      <th>STORM_SPEED</th>\n",
       "      <th>STORM_DIR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.0</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "      <td>15033.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2009.038914</td>\n",
       "      <td>8.160247</td>\n",
       "      <td>15.702122</td>\n",
       "      <td>8.980975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.981800</td>\n",
       "      <td>131.691406</td>\n",
       "      <td>54.421938</td>\n",
       "      <td>-7429.102441</td>\n",
       "      <td>-43797.865562</td>\n",
       "      <td>...</td>\n",
       "      <td>-67676.852325</td>\n",
       "      <td>-67698.088539</td>\n",
       "      <td>-67872.633806</td>\n",
       "      <td>-67797.964146</td>\n",
       "      <td>-77832.682898</td>\n",
       "      <td>-77899.752944</td>\n",
       "      <td>-77953.541209</td>\n",
       "      <td>-77912.947316</td>\n",
       "      <td>10.168496</td>\n",
       "      <td>234.193441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.187875</td>\n",
       "      <td>2.341304</td>\n",
       "      <td>8.666252</td>\n",
       "      <td>6.716750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.349058</td>\n",
       "      <td>19.086857</td>\n",
       "      <td>32.503149</td>\n",
       "      <td>27902.697658</td>\n",
       "      <td>49680.806222</td>\n",
       "      <td>...</td>\n",
       "      <td>46793.468148</td>\n",
       "      <td>46784.069208</td>\n",
       "      <td>46715.952033</td>\n",
       "      <td>46746.058339</td>\n",
       "      <td>41548.057650</td>\n",
       "      <td>41502.381977</td>\n",
       "      <td>41465.478383</td>\n",
       "      <td>41493.640483</td>\n",
       "      <td>5.443846</td>\n",
       "      <td>113.888762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1999.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>-180.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2003.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.600000</td>\n",
       "      <td>121.300003</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>956.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2009.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.400000</td>\n",
       "      <td>130.899994</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>987.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>284.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.600000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>-99999.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2019.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.099998</td>\n",
       "      <td>179.800003</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>1012.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>215.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>360.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             SEASON  ISO_TIME_MONTH  ISO_TIME_DAY  ISO_TIME_HOUR  \\\n",
       "count  15033.000000    15033.000000  15033.000000   15033.000000   \n",
       "mean    2009.038914        8.160247     15.702122       8.980975   \n",
       "std        6.187875        2.341304      8.666252       6.716750   \n",
       "min     1999.000000        1.000000      1.000000       0.000000   \n",
       "25%     2003.000000        7.000000      8.000000       2.000000   \n",
       "50%     2009.000000        8.000000     16.000000       6.000000   \n",
       "75%     2015.000000       10.000000     23.000000      12.000000   \n",
       "max     2019.000000       12.000000     31.000000      23.000000   \n",
       "\n",
       "       ISO_TIME_MIN       USA_LAT       USA_LON      USA_WIND      USA_PRES  \\\n",
       "count       15033.0  15033.000000  15033.000000  15033.000000  15033.000000   \n",
       "mean            0.0     18.981800    131.691406     54.421938  -7429.102441   \n",
       "std             0.0      7.349058     19.086857     32.503149  27902.697658   \n",
       "min             0.0      1.300000   -180.000000     10.000000 -99999.000000   \n",
       "25%             0.0     13.600000    121.300003     30.000000    956.000000   \n",
       "50%             0.0     18.400000    130.899994     45.000000    987.000000   \n",
       "75%             0.0     23.600000    142.000000     75.000000   1000.000000   \n",
       "max             0.0     45.099998    179.800003    170.000000   1012.000000   \n",
       "\n",
       "         USA_R34_NE  ...    USA_R50_NE    USA_R50_SE    USA_R50_SW  \\\n",
       "count  15033.000000  ...  15033.000000  15033.000000  15033.000000   \n",
       "mean  -43797.865562  ... -67676.852325 -67698.088539 -67872.633806   \n",
       "std    49680.806222  ...  46793.468148  46784.069208  46715.952033   \n",
       "min   -99999.000000  ... -99999.000000 -99999.000000 -99999.000000   \n",
       "25%   -99999.000000  ... -99999.000000 -99999.000000 -99999.000000   \n",
       "50%       60.000000  ... -99999.000000 -99999.000000 -99999.000000   \n",
       "75%      120.000000  ...     36.000000     35.000000     30.000000   \n",
       "max      330.000000  ...    200.000000    215.000000    205.000000   \n",
       "\n",
       "         USA_R50_NW    USA_R64_NE    USA_R64_SE    USA_R64_SW    USA_R64_NW  \\\n",
       "count  15033.000000  15033.000000  15033.000000  15033.000000  15033.000000   \n",
       "mean  -67797.964146 -77832.682898 -77899.752944 -77953.541209 -77912.947316   \n",
       "std    46746.058339  41548.057650  41502.381977  41465.478383  41493.640483   \n",
       "min   -99999.000000 -99999.000000 -99999.000000 -99999.000000 -99999.000000   \n",
       "25%   -99999.000000 -99999.000000 -99999.000000 -99999.000000 -99999.000000   \n",
       "50%   -99999.000000 -99999.000000 -99999.000000 -99999.000000 -99999.000000   \n",
       "75%       35.000000 -99999.000000 -99999.000000 -99999.000000 -99999.000000   \n",
       "max      196.000000    135.000000    120.000000    110.000000    107.000000   \n",
       "\n",
       "        STORM_SPEED     STORM_DIR  \n",
       "count  15033.000000  15033.000000  \n",
       "mean      10.168496    234.193441  \n",
       "std        5.443846    113.888762  \n",
       "min        0.000000      0.000000  \n",
       "25%        6.000000    191.000000  \n",
       "50%        9.000000    284.000000  \n",
       "75%       13.000000    310.000000  \n",
       "max       52.000000    360.000000  \n",
       "\n",
       "[8 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_track = best_track.query(\"(SEASON > 1999) or ((SEASON == 1999) and (ISO_TIME_MONTH > 7))\")\n",
    "best_track.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3d12ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15033 entries, 38758 to 53790\n",
      "Data columns (total 25 columns):\n",
      "SID               15033 non-null object\n",
      "SEASON            15033 non-null int32\n",
      "NAME              15033 non-null object\n",
      "ISO_TIME_MONTH    15033 non-null int32\n",
      "ISO_TIME_DAY      15033 non-null int32\n",
      "ISO_TIME_HOUR     15033 non-null int32\n",
      "ISO_TIME_MIN      15033 non-null int32\n",
      "USA_LAT           15033 non-null float32\n",
      "USA_LON           15033 non-null float32\n",
      "USA_WIND          15033 non-null int32\n",
      "USA_PRES          15033 non-null int32\n",
      "USA_R34_NE        15033 non-null int32\n",
      "USA_R34_SE        15033 non-null int32\n",
      "USA_R34_SW        15033 non-null int32\n",
      "USA_R34_NW        15033 non-null int32\n",
      "USA_R50_NE        15033 non-null int32\n",
      "USA_R50_SE        15033 non-null int32\n",
      "USA_R50_SW        15033 non-null int32\n",
      "USA_R50_NW        15033 non-null int32\n",
      "USA_R64_NE        15033 non-null int32\n",
      "USA_R64_SE        15033 non-null int32\n",
      "USA_R64_SW        15033 non-null int32\n",
      "USA_R64_NW        15033 non-null int32\n",
      "STORM_SPEED       15033 non-null int32\n",
      "STORM_DIR         15033 non-null int32\n",
      "dtypes: float32(2), int32(21), object(2)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "best_track.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31716d6e",
   "metadata": {},
   "source": [
    "### Winds, u- and v-components, Part 2\n",
    "\n",
    "The following should be calculated for each record in the subset:\n",
    "\n",
    "- Vertical wind shear\n",
    "  - Upper level, 850mb vs 200mb, 7 degree average\n",
    "  \n",
    "  - Mid level, 500mb vs 850mb, 7 degree average\n",
    "  \n",
    "- U200, 7 degrees\n",
    "\n",
    "- U500 and V500, 7 degrees\n",
    "\n",
    "No more than 15033 values for each variable should be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c88d58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "u200_values = dict()\n",
    "u500_values = dict()\n",
    "v500_values = dict()\n",
    "ulvws_values = dict() # Upper-Lower levels Vertical Wind Shear magnitudes\n",
    "mlvws_values = dict() # Middle-Lower levels Vertical Wind Shear magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2906d54b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1354 data files out of 15033 expected. 147 files are missing. Current year 2001\n",
      "Completed 2844 data files out of 15033 expected. 157 files are missing. Current year 2003\n",
      "Completed 4344 data files out of 15033 expected. 157 files are missing. Current year 2004\n",
      "Completed 5844 data files out of 15033 expected. 157 files are missing. Current year 2006\n",
      "Completed 7343 data files out of 15033 expected. 158 files are missing. Current year 2009\n",
      "Completed 8843 data files out of 15033 expected. 158 files are missing. Current year 2011\n",
      "Completed 10343 data files out of 15033 expected. 158 files are missing. Current year 2013\n",
      "Completed 11843 data files out of 15033 expected. 158 files are missing. Current year 2015\n",
      "Completed 13343 data files out of 15033 expected. 158 files are missing. Current year 2018\n",
      "Completed 14843 data files out of 15033 expected. 158 files are missing. Current year 2019\n"
     ]
    }
   ],
   "source": [
    "num_ok = 0\n",
    "num_expected = len(best_track)\n",
    "num_ng = 0\n",
    "num_gribs_browsed = 0\n",
    "\n",
    "for i in range(len(best_track)):\n",
    "    # select record\n",
    "    record = best_track.iloc[i]\n",
    "    timestamp = datetime.datetime(record[\"SEASON\"], record[\"ISO_TIME_MONTH\"], record[\"ISO_TIME_DAY\"], record[\"ISO_TIME_HOUR\"], 0, 0)\n",
    "    time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")\n",
    "    key_name = \"{0}_{1}\".format(record[\"SID\"], time_string)\n",
    "    # print \"Processing TC:\", key_name\n",
    "    \n",
    "    # many best track records are taken at hours not divisible by 6 (e.g. at 1500)\n",
    "    # interpolation is needed to maximize the utilization of our weather data\n",
    "    # loop start: the GRIB files {loop_start} hours back should be considered\n",
    "    loop_start = 0 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else (record[\"ISO_TIME_HOUR\"] - record[\"ISO_TIME_HOUR\"] % 6)\n",
    "    # loop end: the subsequent file (which is {loop_end - 6} hours later) to consider, 6 if hours divisible by six (no work needed)\n",
    "    loop_end = 6 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else loop_start + 12\n",
    "    # interpolation weight of the earlier record\n",
    "    weight = 1 - (record[\"ISO_TIME_HOUR\"] - (record[\"ISO_TIME_HOUR\"] // 6)*6)/6.0\n",
    "    \n",
    "    # update time reference\n",
    "    number_of_hours_to_go_back = record[\"ISO_TIME_HOUR\"] % 6\n",
    "    timestamp -= datetime.timedelta(hours=number_of_hours_to_go_back)\n",
    "    \n",
    "    u200_list = list()\n",
    "    u500_list = list()\n",
    "    v500_list = list()\n",
    "    ulvws_list = list()\n",
    "    mlvws_list = list()\n",
    "    ng_flag = False\n",
    "    \n",
    "    for k in range(loop_start, loop_end, 6):\n",
    "        time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")   \n",
    "\n",
    "        # prepare before opening file\n",
    "        extension = \"grib1\"\n",
    "        u_var_name = \"U_GRD_3_ISBL\"\n",
    "        v_var_name = \"V_GRD_3_ISBL\"\n",
    "        latlon_suffix = 3 # grib1: lat_3, grib2: lat_0\n",
    "        if timestamp > datetime.datetime(2007, 12, 6, 6, 0, 0):\n",
    "            extension = \"grib2\"\n",
    "            u_var_name = \"UGRD_P0_L100_GLL0\"\n",
    "            v_var_name = \"VGRD_P0_L100_GLL0\"\n",
    "            latlon_suffix = 0\n",
    "\n",
    "        # test file exists\n",
    "        filename = \"./wind/fnl_{0}.{1}\".format(time_string, extension)\n",
    "        num_gribs_browsed += 1\n",
    "        if not os.path.isfile(filename):\n",
    "            # print \"Cannot open file {0}\".format(filename)\n",
    "            ng_flag = True\n",
    "            break\n",
    "\n",
    "        # open file\n",
    "        # print \"Opening file\", filename\n",
    "        wind = Nio.open_file(filename, mode='r')\n",
    "\n",
    "        # U200\n",
    "        center_lat, center_lon = int(round(record[\"USA_LAT\"])), record[\"USA_LON\"]\n",
    "        # negative longitudes do not go well with the addressing\n",
    "        if record[\"USA_LON\"] > 0:\n",
    "            center_lon = int(round(record[\"USA_LON\"]))\n",
    "        elif int(round(record[\"USA_LON\"])) == -180:\n",
    "            center_lon = 180\n",
    "        else:\n",
    "            center_lon = int((round(record[\"USA_LON\"])) + 360) % 180\n",
    "        u200 = wind.variables[u_var_name][\"lv_ISBL0|200\"][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "        u200_avg = np.average(u200)\n",
    "        u200_list.append(u200_avg)\n",
    "        # print \"U200, 7 degree average:\", u200_avg\n",
    "\n",
    "        # U500 and V500\n",
    "        u500 = wind.variables[u_var_name][\"lv_ISBL0|500\"][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "        u500_avg = np.average(u500)\n",
    "        u500_list.append(u500_avg)\n",
    "        # print \"U500, 7 degree average:\", u500_avg\n",
    "        v500 = wind.variables[v_var_name][\"lv_ISBL0|500\"][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "        v500_avg = np.average(v500)\n",
    "        v500_list.append(v500_avg)\n",
    "        # print \"V500, 7 degree average:\",  v500_avg\n",
    "\n",
    "        # vertical wind shear\n",
    "        # get V200, U850 and V850 first\n",
    "        v200 = wind.variables[v_var_name][\"lv_ISBL0|200\"][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "        u850 = wind.variables[u_var_name][\"lv_ISBL0|850\"][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "        v850 = wind.variables[v_var_name][\"lv_ISBL0|850\"][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "        # calculate averages\n",
    "        v200_avg = np.average(v200)\n",
    "        u850_avg = np.average(u850)\n",
    "        v850_avg = np.average(v850)\n",
    "        # upper-lower wind shear\n",
    "        hi_low_shear_u = u200_avg - u850_avg\n",
    "        hi_low_shear_v = v200_avg - v850_avg\n",
    "        hi_low_shear = math.sqrt(hi_low_shear_u ** 2 + hi_low_shear_v ** 2) # magnitude\n",
    "        ulvws_list.append(hi_low_shear)\n",
    "        # print \"Wind shear (upper/lower levels):\", hi_low_shear_u, hi_low_shear_v, hi_low_shear\n",
    "        # mid-lower wind shear\n",
    "        mid_low_shear_u = u500_avg - u850_avg\n",
    "        mid_low_shear_v = v500_avg - v850_avg\n",
    "        mid_low_shear = math.sqrt(mid_low_shear_u ** 2 + mid_low_shear_v ** 2) # magnitude\n",
    "        mlvws_list.append(mid_low_shear)\n",
    "        # print \"Wind shear (mid/lower levels):\", mid_low_shear_u, mid_low_shear_v, mid_low_shear\n",
    "\n",
    "        wind.close()    \n",
    "    \n",
    "    # do not record values if there are missing GRIB files\n",
    "    if ng_flag:\n",
    "        num_ng += 1\n",
    "        continue\n",
    "    \n",
    "    # compute interpolation\n",
    "    # print record[\"ISO_TIME_HOUR\"], loop_start, loop_end, u200_list\n",
    "    if len(u200_list) > 1:\n",
    "        u200_values[key_name] = weight * u200_list[0] + (1-weight) * u200_list[1]\n",
    "        u500_values[key_name] = weight * u500_list[0] + (1-weight) * u500_list[1]\n",
    "        v500_values[key_name] = weight * v500_list[0] + (1-weight) * v500_list[1]\n",
    "        ulvws_values[key_name] = weight * ulvws_list[0] + (1-weight) * ulvws_list[1]\n",
    "        mlvws_values[key_name] = weight * mlvws_list[0] + (1-weight) * mlvws_list[1]\n",
    "    else:\n",
    "        u200_values[key_name] = u200_list[0]\n",
    "        u500_values[key_name] = u500_list[0]\n",
    "        v500_values[key_name] = v500_list[0]\n",
    "        ulvws_values[key_name] = ulvws_list[0]\n",
    "        mlvws_values[key_name] = mlvws_list[0]\n",
    "    \n",
    "    num_ok += 1\n",
    "    \n",
    "    if i % 1500 == 0:\n",
    "        print \"Completed {0} TC records out of {1} expected. {2} records are unusable. Current year {3}\".format(num_ok, num_expected, num_ng, record[\"SEASON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6d11f0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 14875 15033 15165\n",
      "14875\n",
      "14875\n",
      "14875\n",
      "14875\n",
      "14875\n"
     ]
    }
   ],
   "source": [
    "print num_ng, num_ok, num_expected, num_gribs_browsed\n",
    "\n",
    "print len(u200_values)\n",
    "print len(u500_values)\n",
    "print len(v500_values)\n",
    "print len(ulvws_values)\n",
    "print len(mlvws_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d52bd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_csv(u200_values, \"u200.csv\")\n",
    "save_dict_to_csv(u500_values, \"u500.csv\")\n",
    "save_dict_to_csv(v500_values, \"v500.csv\")\n",
    "save_dict_to_csv(ulvws_values, \"ulvms.csv\")\n",
    "save_dict_to_csv(mlvws_values, \"mlvms.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c8a97f",
   "metadata": {},
   "source": [
    "### Relative humidity\n",
    "\n",
    "Phew, last one was hard, let's do something simpler:\n",
    "\n",
    "- 750-800mb relative humidity, 7 degrees\n",
    "\n",
    "- 300-500mb relative humidity, 7 degrees\n",
    "\n",
    "There are 18167 records (533 GRIB1, 17634 GRIB2) available and 15033 TC records to match, but only 8422 humidity values could be computed.\n",
    "\n",
    "Something simpler my donkey. Because of the missing files issue, alternative data sources may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "28aa3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_humid_values = dict()\n",
    "hi_humid_values = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c5c5181",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 TC records out of 15033 expected. 0 records cannot be used. Current year 1999\n",
      "Completed 890 TC records out of 15033 expected. 6611 records cannot be used. Current year 2009\n",
      "Completed 2390 TC records out of 15033 expected. 6611 records cannot be used. Current year 2011\n",
      "Completed 3890 TC records out of 15033 expected. 6611 records cannot be used. Current year 2013\n",
      "Completed 5390 TC records out of 15033 expected. 6611 records cannot be used. Current year 2015\n",
      "Completed 6890 TC records out of 15033 expected. 6611 records cannot be used. Current year 2018\n",
      "Completed 8390 TC records out of 15033 expected. 6611 records cannot be used. Current year 2019\n"
     ]
    }
   ],
   "source": [
    "num_ok, num_ng, num_expected, num_gribs_browsed = 0, 0, len(best_track), 0\n",
    "\n",
    "for i in range(num_expected):\n",
    "    # select record\n",
    "    record = best_track.iloc[i]\n",
    "    timestamp = datetime.datetime(record[\"SEASON\"], record[\"ISO_TIME_MONTH\"], record[\"ISO_TIME_DAY\"], record[\"ISO_TIME_HOUR\"], 0, 0)\n",
    "    time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")\n",
    "    key_name = \"{0}_{1}\".format(record[\"SID\"], time_string)\n",
    "    # print \"Processing TC:\", key_name\n",
    "\n",
    "    center_lat, center_lon = int(round(record[\"USA_LAT\"])), record[\"USA_LON\"]\n",
    "    # negative longitudes do not go well with the addressing\n",
    "    if record[\"USA_LON\"] > 0:\n",
    "        center_lon = int(round(record[\"USA_LON\"]))\n",
    "    elif int(round(record[\"USA_LON\"])) == -180:\n",
    "        center_lon = 180\n",
    "    else:\n",
    "        center_lon = int((round(record[\"USA_LON\"])) + 360) % 180\n",
    "\n",
    "    # many best track records are taken at hours not divisible by 6 (e.g. at 1500)\n",
    "    # interpolation is needed to maximize the utilization of our weather data\n",
    "    # loop start: the GRIB files {loop_start} hours back should be considered\n",
    "    loop_start = 0 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else (record[\"ISO_TIME_HOUR\"] - record[\"ISO_TIME_HOUR\"] % 6)\n",
    "    # loop end: the subsequent file (which is {loop_end - 6} hours later) to consider, 6 if hours divisible by six (no work needed)\n",
    "    loop_end = 6 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else loop_start + 12\n",
    "    # interpolation weight of the earlier record\n",
    "    weight = 1 - (record[\"ISO_TIME_HOUR\"] - (record[\"ISO_TIME_HOUR\"] // 6)*6)/6.0\n",
    "    \n",
    "    # update time reference\n",
    "    number_of_hours_to_go_back = record[\"ISO_TIME_HOUR\"] % 6\n",
    "    timestamp -= datetime.timedelta(hours=number_of_hours_to_go_back)\n",
    "    \n",
    "    hi_humid_list = list()\n",
    "    lo_humid_list = list()\n",
    "    ng_flag = False\n",
    "    \n",
    "    for k in range(loop_start, loop_end, 6):\n",
    "        time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")        \n",
    "        \n",
    "        # prepare before opening file\n",
    "        extension = \"grib1\"\n",
    "        var_name = \"R_H_3_ISBL\"\n",
    "        latlon_suffix = 3 # grib1: lat_3, grib2: lat_0\n",
    "        if timestamp > datetime.datetime(2007, 12, 6, 6, 0, 0):\n",
    "            extension = \"grib2\"\n",
    "            var_name = \"RH_P0_L100_GLL0\"\n",
    "            latlon_suffix = 0\n",
    "\n",
    "        # test file exists\n",
    "        filename = \"./humid/fnl_{0}.{1}\".format(time_string, extension)\n",
    "        num_gribs_browsed += 1\n",
    "        if not os.path.isfile(filename):\n",
    "            # print \"Cannot open file {0}\".format(filename)\n",
    "            ng_flag = True\n",
    "            break\n",
    "\n",
    "        # open file\n",
    "        #print \"Opening file\", filename\n",
    "        humid = Nio.open_file(filename, mode='r')\n",
    "\n",
    "        # 300mb through 500mb\n",
    "        values = list()\n",
    "        for j in range(300,550,50):\n",
    "            grid = humid.variables[var_name][\"lv_ISBL0|{0}\".format(j)][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "            values.append(grid)\n",
    "        values = np.stack(values)\n",
    "        hi_humid = np.average(values)\n",
    "        hi_humid_list.append(hi_humid)\n",
    "\n",
    "        # 750-800mb\n",
    "        values = list()\n",
    "        for j in range(750,800,50):\n",
    "            grid = humid.variables[var_name][\"lv_ISBL0|{0}\".format(j)][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "            values.append(grid)\n",
    "        values = np.stack(values)\n",
    "        lo_humid_list.append(np.average(values))\n",
    "\n",
    "        humid.close()\n",
    "        \n",
    "        timestamp += datetime.timedelta(hours=6)\n",
    "\n",
    "    # do not record values if there are missing GRIB files\n",
    "    if ng_flag:\n",
    "        num_ng += 1\n",
    "        continue\n",
    "    \n",
    "    # compute interpolation\n",
    "    # print record[\"ISO_TIME_HOUR\"], loop_start, loop_end, hi_humid_list\n",
    "    if len(hi_humid_list) > 1:\n",
    "        hi_humid_values[key_name] = weight * hi_humid_list[0] + (1-weight) * hi_humid_list[1]\n",
    "        lo_humid_values[key_name] = weight * lo_humid_list[0] + (1-weight) * lo_humid_list[1]\n",
    "    else:\n",
    "        hi_humid_values[key_name] = hi_humid_list[0]\n",
    "        lo_humid_values[key_name] = lo_humid_list[0]\n",
    "\n",
    "    num_ok += 1\n",
    "\n",
    "    if i % 1500 == 0:\n",
    "        print \"Completed {0} TC records out of {1} expected. {2} records cannot be used. Current year {3}\".format(num_ok, num_expected, num_ng, record[\"SEASON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "43c5b91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6611 8422 15033 15165\n",
      "8422\n",
      "8422\n"
     ]
    }
   ],
   "source": [
    "print num_ng, num_ok, num_expected, num_gribs_browsed\n",
    "\n",
    "print len(lo_humid_values)\n",
    "print len(hi_humid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9b1bb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_csv(lo_humid_values, \"lo_humid.csv\")\n",
    "save_dict_to_csv(hi_humid_values, \"hi_humid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5d98d",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "- surface, 2 degree average\n",
    "  - 11540 GRIB1 and 17634 GRIB2 files (total 29174) are available\n",
    "\n",
    "- 200mb, 7 degree average\n",
    "  - total 29174 files available, same as surface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2903bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_temp_values = dict()\n",
    "temp200_values = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6109d839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1354 TC records out of 15033 expected. 147 records cannot be used. Current year 2001\n",
      "Completed 2844 TC records out of 15033 expected. 157 records cannot be used. Current year 2003\n",
      "Completed 4344 TC records out of 15033 expected. 157 records cannot be used. Current year 2004\n",
      "Completed 5844 TC records out of 15033 expected. 157 records cannot be used. Current year 2006\n",
      "Completed 7343 TC records out of 15033 expected. 158 records cannot be used. Current year 2009\n",
      "Completed 8843 TC records out of 15033 expected. 158 records cannot be used. Current year 2011\n",
      "Completed 10343 TC records out of 15033 expected. 158 records cannot be used. Current year 2013\n",
      "Completed 11843 TC records out of 15033 expected. 158 records cannot be used. Current year 2015\n",
      "Completed 13343 TC records out of 15033 expected. 158 records cannot be used. Current year 2018\n",
      "Completed 14843 TC records out of 15033 expected. 158 records cannot be used. Current year 2019\n"
     ]
    }
   ],
   "source": [
    "num_ok, num_ng, num_expected, num_gribs_browsed = 0, 0, len(best_track), 0\n",
    "\n",
    "for i in range(num_expected):\n",
    "    # select record\n",
    "    record = best_track.iloc[i]\n",
    "    timestamp = datetime.datetime(record[\"SEASON\"], record[\"ISO_TIME_MONTH\"], record[\"ISO_TIME_DAY\"], record[\"ISO_TIME_HOUR\"], 0, 0)\n",
    "    time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")\n",
    "    key_name = \"{0}_{1}\".format(record[\"SID\"], time_string)\n",
    "    # print \"Processing TC:\", key_name\n",
    "\n",
    "    center_lat, center_lon = int(round(record[\"USA_LAT\"])), record[\"USA_LON\"]\n",
    "    # negative longitudes do not go well with the addressing\n",
    "    if record[\"USA_LON\"] > 0:\n",
    "        center_lon = int(round(record[\"USA_LON\"]))\n",
    "    elif int(round(record[\"USA_LON\"])) == -180:\n",
    "        center_lon = 180\n",
    "    else:\n",
    "        center_lon = int((round(record[\"USA_LON\"])) + 360) % 180\n",
    "\n",
    "    # many best track records are taken at hours not divisible by 6 (e.g. at 1500)\n",
    "    # interpolation is needed to maximize the utilization of our weather data\n",
    "    # loop start: the GRIB files {loop_start} hours back should be considered\n",
    "    loop_start = 0 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else (record[\"ISO_TIME_HOUR\"] - record[\"ISO_TIME_HOUR\"] % 6)\n",
    "    # loop end: the subsequent file (which is {loop_end - 6} hours later) to consider, 6 if hours divisible by six (no work needed)\n",
    "    loop_end = 6 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else loop_start + 12\n",
    "    # interpolation weight of the earlier record\n",
    "    weight = 1 - (record[\"ISO_TIME_HOUR\"] - (record[\"ISO_TIME_HOUR\"] // 6)*6)/6.0\n",
    "    \n",
    "    # update time reference\n",
    "    number_of_hours_to_go_back = record[\"ISO_TIME_HOUR\"] % 6\n",
    "    timestamp -= datetime.timedelta(hours=number_of_hours_to_go_back)\n",
    "    \n",
    "    surface_temp_list = list()\n",
    "    temp_200_list = list()\n",
    "    ng_flag = False\n",
    "    \n",
    "    for k in range(loop_start, loop_end, 6):\n",
    "        time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")        \n",
    "        \n",
    "        # prepare before opening file\n",
    "        extension = \"grib1\"\n",
    "        sfc_var_name = \"TMP_3_SFC\"\n",
    "        upper_var_name = \"TMP_3_ISBL\"\n",
    "        latlon_suffix = 3 # grib1: lat_3, grib2: lat_0\n",
    "        if timestamp > datetime.datetime(2007, 12, 6, 6, 0, 0):\n",
    "            extension = \"grib2\"\n",
    "            sfc_var_name = \"TMP_P0_L1_GLL0\"\n",
    "            upper_var_name = \"TMP_P0_L100_GLL0\"\n",
    "            latlon_suffix = 0\n",
    "\n",
    "        ## calculate surface 2 degree average\n",
    "        # test file exists\n",
    "        filename = \"./surface/fnl_{0}.{1}\".format(time_string, extension)\n",
    "        num_gribs_browsed += 1\n",
    "        if not os.path.isfile(filename):\n",
    "            # print \"Cannot open file {0}\".format(filename)\n",
    "            ng_flag = True\n",
    "            break\n",
    "\n",
    "        #print \"Opening file\", filename\n",
    "        surface = Nio.open_file(filename, mode='r')\n",
    "        values = surface.variables[sfc_var_name].get_value()[(center_lat-1+8):(center_lat+1+8),(center_lon-1-53):(center_lon+1-53)]\n",
    "        surface_temp = np.average(values)\n",
    "        surface_temp_list.append(surface_temp)        \n",
    "        surface.close()\n",
    "        \n",
    "        ## calculate 200 hpa 7 degree average\n",
    "        # test file exists\n",
    "        filename = \"./temp/fnl_{0}.{1}\".format(time_string, extension)\n",
    "        num_gribs_browsed += 1\n",
    "        if not os.path.isfile(filename):\n",
    "            # print \"Cannot open file {0}\".format(filename)\n",
    "            ng_flag = True\n",
    "            break\n",
    "\n",
    "        #print \"Opening file\", filename\n",
    "        temp = Nio.open_file(filename, mode='r')\n",
    "        values = temp.variables[upper_var_name][\"lv_ISBL0|200\"][(center_lat-6+8):(center_lat+6+8),(center_lon-6-53):(center_lon+6-53)]\n",
    "        temp_200 = np.average(values)\n",
    "        temp_200_list.append(temp_200)        \n",
    "        temp.close()\n",
    "        \n",
    "        timestamp += datetime.timedelta(hours=6)\n",
    "\n",
    "    # do not record values if there are missing GRIB files\n",
    "    if ng_flag:\n",
    "        num_ng += 1\n",
    "        continue\n",
    "    \n",
    "    # compute interpolation\n",
    "    # print record[\"ISO_TIME_HOUR\"], loop_start, loop_end, surface_temp_list\n",
    "    if len(surface_temp_list) > 1:\n",
    "        surface_temp_values[key_name] = weight * surface_temp_list[0] + (1-weight) * surface_temp_list[1]\n",
    "        temp200_values[key_name] = weight * temp_200_list[0] + (1-weight) * temp_200_list[1]\n",
    "    else:\n",
    "        surface_temp_values[key_name] = surface_temp_list[0]\n",
    "        temp200_values[key_name] = temp_200_list[0]\n",
    "\n",
    "    num_ok += 1\n",
    "\n",
    "    if i % 1500 == 0:\n",
    "        print \"Completed {0} TC records out of {1} expected. {2} records cannot be used. Current year {3}\".format(num_ok, num_expected, num_ng, record[\"SEASON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b89c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 14875 15033 30172\n",
      "14875\n",
      "14875\n"
     ]
    }
   ],
   "source": [
    "print num_ng, num_ok, num_expected, num_gribs_browsed\n",
    "\n",
    "print len(surface_temp_values)\n",
    "print len(temp200_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af1c007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_csv(surface_temp_values, \"temp_surface.csv\")\n",
    "save_dict_to_csv(temp200_values, \"temp200.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87347624",
   "metadata": {},
   "source": [
    "### Vorticity\n",
    "\n",
    "850mb relative vorticity (7 degrees average) is needed. The given data is in *absolute* vorticity, where an extra term for Coriolis force is present. This can be manually taken out. However, the resulting values look unreasonably low compared to known relative vorticity values, so absolute vorticity is kept.\n",
    "\n",
    "18167 data files are available (533 grib1 and 17634 grib2).\n",
    "\n",
    "Because of the missing files issue, alternative data sources may be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c8d394f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1 TC records out of 15033 expected. 0 records cannot be used. Current year 1999\n",
      "Completed 890 TC records out of 15033 expected. 6611 records cannot be used. Current year 2009\n",
      "Completed 2390 TC records out of 15033 expected. 6611 records cannot be used. Current year 2011\n",
      "Completed 3890 TC records out of 15033 expected. 6611 records cannot be used. Current year 2013\n",
      "Completed 5390 TC records out of 15033 expected. 6611 records cannot be used. Current year 2015\n",
      "Completed 6890 TC records out of 15033 expected. 6611 records cannot be used. Current year 2018\n",
      "Completed 8390 TC records out of 15033 expected. 6611 records cannot be used. Current year 2019\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def coriolis_param(lat):\n",
    "    '''Computes the Coriolis parameter from the given latitude (in degrees)'''\n",
    "    omega = 7.2921e-05\n",
    "    return 2*omega*math.sin(math.radians(lat))\n",
    "\n",
    "vort_values = dict()\n",
    "\n",
    "num_ok, num_ng, num_expected, num_gribs_browsed = 0, 0, len(best_track), 0\n",
    "\n",
    "for i in range(num_expected):\n",
    "    # select record\n",
    "    record = best_track.iloc[i]\n",
    "    timestamp = datetime.datetime(record[\"SEASON\"], record[\"ISO_TIME_MONTH\"], record[\"ISO_TIME_DAY\"], record[\"ISO_TIME_HOUR\"], 0, 0)\n",
    "    time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")\n",
    "    key_name = \"{0}_{1}\".format(record[\"SID\"], time_string)\n",
    "    # print \"Processing TC:\", key_name\n",
    "\n",
    "    center_lat, center_lon = int(round(record[\"USA_LAT\"])), record[\"USA_LON\"]\n",
    "    # negative longitudes do not go well with the addressing\n",
    "    if record[\"USA_LON\"] > 0:\n",
    "        center_lon = int(round(record[\"USA_LON\"]))\n",
    "    elif int(round(record[\"USA_LON\"])) == -180:\n",
    "        center_lon = 180\n",
    "    else:\n",
    "        center_lon = int((round(record[\"USA_LON\"])) + 360) % 180\n",
    "\n",
    "    # many best track records are taken at hours not divisible by 6 (e.g. at 1500)\n",
    "    # interpolation is needed to maximize the utilization of our weather data\n",
    "    # loop start: the GRIB files {loop_start} hours back should be considered\n",
    "    loop_start = 0 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else (record[\"ISO_TIME_HOUR\"] - record[\"ISO_TIME_HOUR\"] % 6)\n",
    "    # loop end: the subsequent file (which is {loop_end - 6} hours later) to consider, 6 if hours divisible by six (no work needed)\n",
    "    loop_end = 6 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else loop_start + 12\n",
    "    # interpolation weight of the earlier record\n",
    "    weight = 1 - (record[\"ISO_TIME_HOUR\"] - (record[\"ISO_TIME_HOUR\"] // 6)*6)/6.0\n",
    "    \n",
    "    # update time reference\n",
    "    number_of_hours_to_go_back = record[\"ISO_TIME_HOUR\"] % 6\n",
    "    timestamp -= datetime.timedelta(hours=number_of_hours_to_go_back)\n",
    "    \n",
    "    vort850_list = list()\n",
    "    ng_flag = False\n",
    "    \n",
    "    for k in range(loop_start, loop_end, 6):\n",
    "        time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")        \n",
    "        \n",
    "        # prepare before opening file\n",
    "        extension = \"grib1\"\n",
    "        var_name = \"ABS_V_3_ISBL\"\n",
    "        latlon_suffix = 3 # grib1: lat_3, grib2: lat_0\n",
    "        if timestamp > datetime.datetime(2007, 12, 6, 6, 0, 0):\n",
    "            extension = \"grib2\"\n",
    "            var_name = \"ABSV_P0_L100_GLL0\"\n",
    "            latlon_suffix = 0\n",
    "\n",
    "        ## calculate surface 2 degree average\n",
    "        # test file exists\n",
    "        filename = \"./vort/fnl_{0}.{1}\".format(time_string, extension)\n",
    "        num_gribs_browsed += 1\n",
    "        if not os.path.isfile(filename):\n",
    "            # print \"Cannot open file {0}\".format(filename)\n",
    "            ng_flag = True\n",
    "            break\n",
    "\n",
    "        #print \"Opening file\", filename\n",
    "        vort = Nio.open_file(filename, mode='r')\n",
    "        values = vort.variables[var_name].get_value()[(center_lat-1+8):(center_lat+1+8),(center_lon-1-53):(center_lon+1-53)]\n",
    "        vort.close()\n",
    "        \n",
    "        '''for x in range(values.shape[0]):\n",
    "            for y in range(values.shape[1]):\n",
    "                values[x,y] = values[x,y] - coriolis_param(y + (center_lat-6+8))'''\n",
    "                \n",
    "        vort850 = np.average(values)\n",
    "        vort850_list.append(vort850)\n",
    "        \n",
    "        timestamp += datetime.timedelta(hours=6)\n",
    "\n",
    "    # do not record values if there are missing GRIB files\n",
    "    if ng_flag:\n",
    "        num_ng += 1\n",
    "        continue\n",
    "    \n",
    "    # compute interpolation\n",
    "    # print record[\"ISO_TIME_HOUR\"], loop_start, loop_end, vort850_list\n",
    "    if len(vort850_list) > 1:\n",
    "        vort_values[key_name] = weight * vort850_list[0] + (1-weight) * vort850_list[1]\n",
    "    else:\n",
    "        vort_values[key_name] = vort850_list[0]\n",
    "\n",
    "    num_ok += 1\n",
    "\n",
    "    if i % 1500 == 0:\n",
    "        print \"Completed {0} TC records out of {1} expected. {2} records cannot be used. Current year {3}\".format(num_ok, num_expected, num_ng, record[\"SEASON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "80c4f693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6611 8422 15033 15165\n",
      "8422\n"
     ]
    }
   ],
   "source": [
    "print num_ng, num_ok, num_expected, num_gribs_browsed\n",
    "\n",
    "print len(vort_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d95a8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict_to_csv(vort_values, \"vort850.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa65dd",
   "metadata": {},
   "source": [
    "### Potential Temperature\n",
    "\n",
    "The original plan is to calculate it for 925mb level, but the data comes in sigma level 0.995, i.e. the pressure level is 0.995 that of the surface pressure. Therefore the 925mb level potential temperature cannot be calculated. Instead, a direct 2-degree average will be taken.\n",
    "\n",
    "11540 GRIB1 and 17633 GRIB2 files are available, adding up to a total of 29173 files. Interestingly enough, potential temperature is a variable that is rather hard to come by in other atmospheric reanalysis archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01b796e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 1354 TC records out of 15033 expected. 147 records cannot be used. Current year 2001\n",
      "Completed 2844 TC records out of 15033 expected. 157 records cannot be used. Current year 2003\n",
      "Completed 4344 TC records out of 15033 expected. 157 records cannot be used. Current year 2004\n",
      "Completed 5844 TC records out of 15033 expected. 157 records cannot be used. Current year 2006\n",
      "Completed 7343 TC records out of 15033 expected. 158 records cannot be used. Current year 2009\n",
      "Completed 8843 TC records out of 15033 expected. 158 records cannot be used. Current year 2011\n",
      "Completed 10341 TC records out of 15033 expected. 160 records cannot be used. Current year 2013\n",
      "Completed 11841 TC records out of 15033 expected. 160 records cannot be used. Current year 2015\n",
      "Completed 13341 TC records out of 15033 expected. 160 records cannot be used. Current year 2018\n",
      "Completed 14841 TC records out of 15033 expected. 160 records cannot be used. Current year 2019\n"
     ]
    }
   ],
   "source": [
    "pott_values = dict()\n",
    "\n",
    "num_ok, num_ng, num_expected, num_gribs_browsed = 0, 0, len(best_track), 0\n",
    "\n",
    "for i in range(num_expected):\n",
    "    # select record\n",
    "    record = best_track.iloc[i]\n",
    "    timestamp = datetime.datetime(record[\"SEASON\"], record[\"ISO_TIME_MONTH\"], record[\"ISO_TIME_DAY\"], record[\"ISO_TIME_HOUR\"], 0, 0)\n",
    "    time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")\n",
    "    key_name = \"{0}_{1}\".format(record[\"SID\"], time_string)\n",
    "    # print \"Processing TC:\", key_name\n",
    "\n",
    "    center_lat, center_lon = int(round(record[\"USA_LAT\"])), record[\"USA_LON\"]\n",
    "    # negative longitudes do not go well with the addressing\n",
    "    if record[\"USA_LON\"] > 0:\n",
    "        center_lon = int(round(record[\"USA_LON\"]))\n",
    "    elif int(round(record[\"USA_LON\"])) == -180:\n",
    "        center_lon = 180\n",
    "    else:\n",
    "        center_lon = int((round(record[\"USA_LON\"])) + 360) % 180\n",
    "\n",
    "    # many best track records are taken at hours not divisible by 6 (e.g. at 1500)\n",
    "    # interpolation is needed to maximize the utilization of our weather data\n",
    "    # loop start: the GRIB files {loop_start} hours back should be considered\n",
    "    loop_start = 0 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else (record[\"ISO_TIME_HOUR\"] - record[\"ISO_TIME_HOUR\"] % 6)\n",
    "    # loop end: the subsequent file (which is {loop_end - 6} hours later) to consider, 6 if hours divisible by six (no work needed)\n",
    "    loop_end = 6 if record[\"ISO_TIME_HOUR\"] % 6 == 0 else loop_start + 12\n",
    "    # interpolation weight of the earlier record\n",
    "    weight = 1 - (record[\"ISO_TIME_HOUR\"] - (record[\"ISO_TIME_HOUR\"] // 6)*6)/6.0\n",
    "    \n",
    "    # update time reference\n",
    "    number_of_hours_to_go_back = record[\"ISO_TIME_HOUR\"] % 6\n",
    "    timestamp -= datetime.timedelta(hours=number_of_hours_to_go_back)\n",
    "    \n",
    "    pott_list = list()\n",
    "    ng_flag = False\n",
    "    \n",
    "    for k in range(loop_start, loop_end, 6):\n",
    "        time_string = timestamp.strftime(\"%Y%m%d_%H_%M\")        \n",
    "        \n",
    "        # prepare before opening file\n",
    "        extension = \"grib1\"\n",
    "        var_name = \"POT_3_SIGL\"\n",
    "        latlon_suffix = 3 # grib1: lat_3, grib2: lat_0\n",
    "        if timestamp > datetime.datetime(2007, 12, 6, 6, 0, 0):\n",
    "            extension = \"grib2\"\n",
    "            var_name = \"POT_P0_L104_GLL0\"\n",
    "            latlon_suffix = 0\n",
    "\n",
    "        ## calculate surface 2 degree average\n",
    "        # test file exists\n",
    "        filename = \"./potential_temp/fnl_{0}.{1}\".format(time_string, extension)\n",
    "        num_gribs_browsed += 1\n",
    "        if not os.path.isfile(filename):\n",
    "            # print \"Cannot open file {0}\".format(filename)\n",
    "            ng_flag = True\n",
    "            break\n",
    "\n",
    "        #print \"Opening file\", filename\n",
    "        pott = Nio.open_file(filename, mode='r')\n",
    "        values = pott.variables[var_name].get_value()[(center_lat-1+8):(center_lat+1+8),(center_lon-1-53):(center_lon+1-53)]\n",
    "        pott.close()\n",
    "                \n",
    "        pott_avg = np.average(values)\n",
    "        pott_list.append(pott_avg)\n",
    "        \n",
    "        timestamp += datetime.timedelta(hours=6)\n",
    "\n",
    "    # do not record values if there are missing GRIB files\n",
    "    if ng_flag:\n",
    "        num_ng += 1\n",
    "        continue\n",
    "    \n",
    "    # compute interpolation\n",
    "    # print record[\"ISO_TIME_HOUR\"], loop_start, loop_end, pott_list\n",
    "    if len(pott_list) > 1:\n",
    "        pott_values[key_name] = weight * pott_list[0] + (1-weight) * pott_list[1]\n",
    "    else:\n",
    "        pott_values[key_name] = pott_list[0]\n",
    "\n",
    "    num_ok += 1\n",
    "\n",
    "    if i % 1500 == 0:\n",
    "        print \"Completed {0} TC records out of {1} expected. {2} records cannot be used. Current year {3}\".format(num_ok, num_expected, num_ng, record[\"SEASON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b7fe36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 14873 15033 15165\n",
      "14873\n"
     ]
    }
   ],
   "source": [
    "print num_ng, num_ok, num_expected, num_gribs_browsed\n",
    "\n",
    "print len(pott_values)\n",
    "\n",
    "save_dict_to_csv(pott_values, \"pott.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
